#!/usr/bin/env python3
"""
ç”³ä¸‡è¡Œä¸šä¼°å€¼åˆ†ææ‰§è¡Œè„šæœ¬

åŠŸèƒ½ï¼š
1. æ‰§è¡Œ L1ã€L2ã€L3 ä¸‰ä¸ªçº§åˆ«çš„è¡Œä¸šä¼°å€¼åˆ†æï¼ˆ30å¹´å†å²æ•°æ®ï¼‰
2. æ¸…ç†7å¤©å‰çš„åˆ†ææ–‡ä»¶
3. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
"""

import os
import sys
import argparse
import subprocess
from datetime import datetime, timedelta
import pandas as pd
import glob


def run_analysis(level, years=30, output_dir="./output"):
    """
    æ‰§è¡ŒæŒ‡å®šçº§åˆ«çš„è¡Œä¸šä¼°å€¼åˆ†æ

    å‚æ•°:
        level: L1/L2/L3
        years: å†å²å¹´æ•°
        output_dir: è¾“å‡ºç›®å½•

    è¿”å›:
        str: ç”Ÿæˆçš„CSVæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå¤±è´¥è¿”å›None
    """
    script_path = os.path.join(os.path.dirname(__file__), "industry_pe_pb_sw.py")

    if not os.path.exists(script_path):
        print(f"âŒ è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: {script_path}")
        return None

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # æ‰§è¡Œè„šæœ¬
    cmd = [sys.executable, script_path, "--level", level, "--years", str(years), "--output", output_dir]

    print(f"\n{'='*70}")
    print(f"æ‰§è¡Œ {level} è¡Œä¸šåˆ†æ")
    print(f"{'='*70}")

    try:
        result = subprocess.run(cmd, check=True, capture_output=False)
    except subprocess.CalledProcessError as e:
        print(f"âŒ {level} è¡Œä¸šåˆ†ææ‰§è¡Œå¤±è´¥")
        return None

    # æŸ¥æ‰¾æœ€æ–°ç”Ÿæˆçš„æ–‡ä»¶
    pattern = os.path.join(output_dir, f"industry_pe_pb_sw_{level}_*.csv")
    files = glob.glob(pattern)

    if not files:
        print(f"âš ï¸  æœªæ‰¾åˆ° {level} åˆ†æç»“æœæ–‡ä»¶")
        return None

    # è¿”å›æœ€æ–°æ–‡ä»¶
    latest_file = max(files, key=os.path.getmtime)
    print(f"âœ… {level} åˆ†æå®Œæˆ: {latest_file}")
    return latest_file


def cleanup_old_files(output_dir="./output", days=7):
    """
    æ¸…ç†æŒ‡å®šå¤©æ•°å‰çš„æ—§æ–‡ä»¶

    å‚æ•°:
        output_dir: è¾“å‡ºç›®å½•
        days: ä¿ç•™å¤©æ•°
    """
    cutoff_time = datetime.now() - timedelta(days=days)

    pattern = os.path.join(output_dir, "industry_pe_pb_sw_*.csv")
    files = glob.glob(pattern)

    cleaned_count = 0
    for file_path in files:
        file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
        if file_time < cutoff_time:
            os.remove(file_path)
            cleaned_count += 1
            print(f"ğŸ—‘ï¸  åˆ é™¤æ—§æ–‡ä»¶: {os.path.basename(file_path)}")

    if cleaned_count > 0:
        print(f"\nâœ… å·²æ¸…ç† {cleaned_count} ä¸ªè¶…è¿‡ {days} å¤©çš„æ—§æ–‡ä»¶")
    else:
        print(f"\nâ„¹ï¸  æ²¡æœ‰è¶…è¿‡ {days} å¤©çš„æ—§æ–‡ä»¶éœ€è¦æ¸…ç†")


def generate_summary_report(l1_file, l2_file, l3_file, output_dir="./output"):
    """
    ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š

    å‚æ•°:
        l1_file: L1åˆ†æç»“æœæ–‡ä»¶è·¯å¾„
        l2_file: L2åˆ†æç»“æœæ–‡ä»¶è·¯å¾„
        l3_file: L3åˆ†æç»“æœæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•

    è¿”å›:
        str: ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    report_date = datetime.now().strftime('%Y%m%d')
    report_file = os.path.join(output_dir, f"valuation_report_{report_date}.md")

    with open(report_file, 'w', encoding='utf-8') as f:
        # æ ‡é¢˜
        f.write(f"# ç”³ä¸‡è¡Œä¸šä¼°å€¼åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**æ•°æ®æ¥æº**: Tushare sw_daily æ¥å£\n\n")
        f.write(f"**å†å²è·¨åº¦**: 30å¹´\n\n")
        f.write("---\n\n")

        # ç¬¬ä¸€éƒ¨åˆ†ï¼šæœ¬æ—¥ä¼°å€¼æ¦‚è§ˆ
        f.write("## ç¬¬ä¸€éƒ¨åˆ†ï¼šæœ¬æ—¥ä¼°å€¼æ¦‚è§ˆ\n\n")

        for level, file_path, level_name in [
            ('L1', l1_file, 'ä¸€çº§è¡Œä¸š'),
            ('L2', l2_file, 'äºŒçº§è¡Œä¸š'),
            ('L3', l3_file, 'ä¸‰çº§è¡Œä¸š')
        ]:
            if file_path is None or not os.path.exists(file_path):
                f.write(f"### {level_name} - æ— æ•°æ®\n\n")
                continue

            df = pd.read_csv(file_path)
            df = df.sort_values('pe_percentile', ascending=True, na_position='last')

            f.write(f"### {level_name}\n\n")

            # Markdown è¡¨æ ¼
            f.write("| æ’å | è¡Œä¸šä»£ç  | è¡Œä¸šåç§° | PE | PEç™¾åˆ†ä½ | PB | PBç™¾åˆ†ä½ | æ•°æ®ç‚¹ | æœ€æ–°æ—¥æœŸ |\n")
            f.write("|------|----------|----------|----|---------|----|---------|-------|---------|\n")

            for idx, row in df.iterrows():
                pe_str = f"{row['pe']:.2f}" if pd.notna(row['pe']) else "N/A"
                pb_str = f"{row['pb']:.2f}" if pd.notna(row['pb']) else "N/A"
                pe_pct_str = f"{row['pe_percentile']:.1f}%" if pd.notna(row['pe_percentile']) else "N/A"
                pb_pct_str = f"{row['pb_percentile']:.1f}%" if pd.notna(row['pb_percentile']) else "N/A"
                latest_date_str = str(row['latest_date']) if pd.notna(row['latest_date']) else "N/A"

                # æ ‡è®°é«˜ä¼°/ä½ä¼°
                pe_valuation = ""
                if pd.notna(row['pe_percentile']):
                    if row['pe_percentile'] <= 20:
                        pe_valuation = " ğŸŸ¢"
                    elif row['pe_percentile'] >= 80:
                        pe_valuation = " ğŸ”´"

                pb_valuation = ""
                if pd.notna(row['pb_percentile']):
                    if row['pb_percentile'] <= 20:
                        pb_valuation = " ğŸŸ¢"
                    elif row['pb_percentile'] >= 80:
                        pb_valuation = " ğŸ”´"

                f.write(f"| {int(idx)+1} | {row['index_code']} | {row['index_name']} | {pe_str} | {pe_pct_str}{pe_valuation} | {pb_str} | {pb_pct_str}{pb_valuation} | {row['sample_count']} | {latest_date_str} |\n")

            # æ±‡æ€»ç»Ÿè®¡
            f.write(f"\n**{level_name}æ±‡æ€»**ï¼š\n\n")
            pe_low = df[df['pe_percentile'] <= 20].shape[0]
            pe_high = df[df['pe_percentile'] >= 80].shape[0]
            pb_low = df[df['pb_percentile'] <= 20].shape[0]
            pb_high = df[df['pb_percentile'] >= 80].shape[0]
            pe_median = df['pe_percentile'].median()
            pb_median = df['pb_percentile'].median()

            f.write(f"- ä½ä¼°è¡Œä¸šï¼ˆPE â‰¤ 20%ï¼‰ï¼š{pe_low} ä¸ª\n")
            f.write(f"- é«˜ä¼°è¡Œä¸šï¼ˆPE â‰¥ 80%ï¼‰ï¼š{pe_high} ä¸ª\n")
            f.write(f"- ä½ä¼°è¡Œä¸šï¼ˆPB â‰¤ 20%ï¼‰ï¼š{pb_low} ä¸ª\n")
            f.write(f"- é«˜ä¼°è¡Œä¸šï¼ˆPB â‰¥ 80%ï¼‰ï¼š{pb_high} ä¸ª\n")
            f.write(f"- PE ç™¾åˆ†ä½ä¸­ä½æ•°ï¼š{pe_median:.1f}%\n")
            f.write(f"- PB ç™¾åˆ†ä½ä¸­ä½æ•°ï¼š{pb_median:.1f}%\n\n")

        # ç¬¬äºŒéƒ¨åˆ†ï¼šä¸ƒæ—¥è¶‹åŠ¿å˜åŒ–
        f.write("---\n\n")
        f.write("## ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸ƒæ—¥è¶‹åŠ¿å˜åŒ–\n\n")

        f.write("### PE ç™¾åˆ†ä½å˜åŒ–ï¼ˆè¶…è¿‡ 10%ï¼‰\n\n")
        f.write("| è¡Œä¸šçº§åˆ« | è¡Œä¸šåç§° | å½“å‰ç™¾åˆ†ä½ | 7æ—¥å‰ç™¾åˆ†ä½ | å˜åŒ– |\n")
        f.write("|----------|----------|-----------|------------|------|\n")

        has_pe_change = False
        for level, file_path, level_name in [
            ('L1', l1_file, 'ä¸€çº§è¡Œä¸š'),
            ('L2', l2_file, 'äºŒçº§è¡Œä¸š'),
            ('L3', l3_file, 'ä¸‰çº§è¡Œä¸š')
        ]:
            if file_path is None or not os.path.exists(file_path):
                continue

            # è·å–å½“å‰æ•°æ®
            df_current = pd.read_csv(file_path)

            # æŸ¥æ‰¾7å¤©å‰çš„æ•°æ®
            cutoff_time = datetime.now() - timedelta(days=7)
            pattern = os.path.join(output_dir, f"industry_pe_pb_sw_{level}_*.csv")
            old_files = [f for f in glob.glob(pattern)
                        if datetime.fromtimestamp(os.path.getmtime(f)) < cutoff_time]

            if old_files:
                df_old_file = max(old_files, key=os.path.getmtime)
                df_old = pd.read_csv(df_old_file)

                # æ¯”è¾ƒå˜åŒ–
                merged = pd.merge(df_current[['index_name', 'pe_percentile']],
                                 df_old[['index_name', 'pe_percentile']],
                                 on='index_name', suffixes=('_current', '_old'))

                # æ‰¾å‡ºå˜åŒ–è¶…è¿‡10%çš„
                merged['pe_change'] = merged['pe_percentile_current'] - merged['pe_percentile_old']
                large_changes = merged[abs(merged['pe_change']) >= 10]

                for _, row in large_changes.iterrows():
                    change_arrow = "ğŸ“ˆ" if row['pe_change'] > 0 else "ğŸ“‰"
                    f.write(f"| {level_name} | {row['index_name']} | {row['pe_percentile_current']:.1f}% | {row['pe_percentile_old']:.1f}% | {change_arrow} {row['pe_change']:+.1f}% |\n")
                    has_pe_change = True

        if not has_pe_change:
            f.write("| - | æ— æ˜¾è‘—å˜åŒ– | - | - | - |\n")

        f.write("\n### PB ç™¾åˆ†ä½å˜åŒ–ï¼ˆè¶…è¿‡ 10%ï¼‰\n\n")
        f.write("| è¡Œä¸šçº§åˆ« | è¡Œä¸šåç§° | å½“å‰ç™¾åˆ†ä½ | 7æ—¥å‰ç™¾åˆ†ä½ | å˜åŒ– |\n")
        f.write("|----------|----------|-----------|------------|------|\n")

        has_pb_change = False
        for level, file_path, level_name in [
            ('L1', l1_file, 'ä¸€çº§è¡Œä¸š'),
            ('L2', l2_file, 'äºŒçº§è¡Œä¸š'),
            ('L3', l3_file, 'ä¸‰çº§è¡Œä¸š')
        ]:
            if file_path is None or not os.path.exists(file_path):
                continue

            # è·å–å½“å‰æ•°æ®
            df_current = pd.read_csv(file_path)

            # æŸ¥æ‰¾7å¤©å‰çš„æ•°æ®
            cutoff_time = datetime.now() - timedelta(days=7)
            pattern = os.path.join(output_dir, f"industry_pe_pb_sw_{level}_*.csv")
            old_files = [f for f in glob.glob(pattern)
                        if datetime.fromtimestamp(os.path.getmtime(f)) < cutoff_time]

            if old_files:
                df_old_file = max(old_files, key=os.path.getmtime)
                df_old = pd.read_csv(df_old_file)

                # æ¯”è¾ƒå˜åŒ–
                merged = pd.merge(df_current[['index_name', 'pb_percentile']],
                                 df_old[['index_name', 'pb_percentile']],
                                 on='index_name', suffixes=('_current', '_old'))

                # æ‰¾å‡ºå˜åŒ–è¶…è¿‡10%çš„
                merged['pb_change'] = merged['pb_percentile_current'] - merged['pb_percentile_old']
                large_changes = merged[abs(merged['pb_change']) >= 10]

                for _, row in large_changes.iterrows():
                    change_arrow = "ğŸ“ˆ" if row['pb_change'] > 0 else "ğŸ“‰"
                    f.write(f"| {level_name} | {row['index_name']} | {row['pb_percentile_current']:.1f}% | {row['pb_percentile_old']:.1f}% | {change_arrow} {row['pb_change']:+.1f}% |\n")
                    has_pb_change = True

        if not has_pb_change:
            f.write("| - | æ— æ˜¾è‘—å˜åŒ– | - | - | - |\n")

        # ç‰¹åˆ«æé†’
        f.write("\n### âš ï¸ ç‰¹åˆ«æé†’\n\n")
        f.write("- ğŸŸ¢ è¡¨ç¤ºä½ä¼°ï¼ˆç™¾åˆ†ä½ â‰¤ 20%ï¼‰ï¼Œå¯èƒ½å­˜åœ¨æŠ•èµ„æœºä¼š\n")
        f.write("- ğŸ”´ è¡¨ç¤ºé«˜ä¼°ï¼ˆç™¾åˆ†ä½ â‰¥ 80%ï¼‰ï¼Œéœ€è¦æ³¨æ„é£é™©\n")
        f.write("- ä¼°å€¼åˆ¤æ–­éœ€ç»“åˆè¡Œä¸šç‰¹æ€§ï¼Œæˆé•¿æ€§è¡Œä¸šå¯èƒ½é•¿æœŸç»´æŒè¾ƒé«˜ä¼°å€¼\n")
        f.write("- ç™¾åˆ†ä½å˜åŒ–è¶…è¿‡ 10% éœ€è¦ç‰¹åˆ«å…³æ³¨\n\n")

        f.write("---\n\n")
        f.write(f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")

    print(f"\n{'='*70}")
    print(f"âœ… æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    print(f"{'='*70}")

    return report_file


def main():
    parser = argparse.ArgumentParser(description='ç”³ä¸‡è¡Œä¸šä¼°å€¼åˆ†ææ‰§è¡Œè„šæœ¬')
    parser.add_argument('--token', '-t', type=str, help='Tushare Token (ä¸æŒ‡å®šåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼)')
    parser.add_argument('--output', '-o', type=str, default='./output', help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤ ./output)')
    parser.add_argument('--cleanup-days', '-c', type=int, default=365, help='æ¸…ç†å¤šå°‘å¤©å‰çš„æ—§æ–‡ä»¶ (é»˜è®¤ 365)')
    args = parser.parse_args()

    # è®¾ç½® token
    if args.token:
        os.environ['TUSHARE_TOKEN'] = args.token
        print(f"âœ… ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„ Token")

    # è¾“å‡ºç›®å½•
    output_dir = args.output

    # æ­¥éª¤1ï¼šæ¸…ç†æ—§æ–‡ä»¶
    print("\n" + "="*70)
    print("æ­¥éª¤1ï¼šæ¸…ç†æ—§æ–‡ä»¶")
    print("="*70)
    cleanup_old_files(output_dir, args.cleanup_days)

    # æ­¥éª¤2ï¼šæ‰§è¡Œä¸‰çº§åˆ†æ
    print("\n" + "="*70)
    print("æ­¥éª¤2ï¼šæ‰§è¡Œè¡Œä¸šä¼°å€¼åˆ†æ")
    print("="*70)

    l1_file = run_analysis('L1', years=30, output_dir=output_dir)
    l2_file = run_analysis('L2', years=30, output_dir=output_dir)
    l3_file = run_analysis('L3', years=30, output_dir=output_dir)

    # æ­¥éª¤3ï¼šç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print("\n" + "="*70)
    print("æ­¥éª¤3ï¼šç”Ÿæˆæ±‡æ€»æŠ¥å‘Š")
    print("="*70)

    generate_summary_report(l1_file, l2_file, l3_file, output_dir)

    print("\n" + "="*70)
    print("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print("="*70)


if __name__ == '__main__':
    main()
